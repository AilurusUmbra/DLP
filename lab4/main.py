from __future__ import unicode_literals, print_function, division
import os
import torch
import copy
import matplotlib.pyplot as plt
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
from model import EncoderRNN, DecoderRNN
from train import trainIters, evaluateAll, evaluate
from dataloader import DataTrans

"""========================================================================================
The main.py includes the following template functions:

1. Encoder, decoder
2. Training function
3. BLEU-4 score function

You have to modify them to complete the lab.
In addition, there are still other functions that you have to 
implement by yourself.

1. Your own dataloader (design in your own way, not necessary Pytorch Dataloader)
2. Output your results (BLEU-4 score, correction words)
3. Plot loss/score
4. Load/save weights 
========================================================================================"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SOS_token = 0
EOS_token = 1
#----------Hyper Parameters----------#
hidden_size = 512  # LSTM hidden size
vocab_size = 29  # The number of vocabulary:vocab_size==input_size ,containing:SOS,EOS,UNK,a-z
teacher_forcing_ratio = 0.1
LR = 0.05
verbose = True
load_pretrained = True 

#compute BLEU-4 score
def compute_bleu(output, reference):
    """
    output = 'varable'  #The word generated by your model
    reference = 'variable'  #The target word
    """
    cc = SmoothingFunction()
    return sentence_bleu([reference], output,weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=cc.method1)


if __name__=='__main__':
    """
    load training data
    """
    # training data
    data = DataTrans()
    training_list,_ = data.build_training_set(path='train.json')
    training_tensor_list = []
    # convert list to tensor
    for training_pair in training_list:
        input_tensor = torch.tensor(training_pair[0], device=device).view(-1, 1)
        target_tensor = torch.tensor(training_pair[1], device=device).view(-1, 1)
        training_tensor_list.append((input_tensor, target_tensor))
    # testing data
    testing_list,testing_input=data.build_training_set(path='test.json')
    testing_tensor_list=[]
    # convert list to tensor
    for testing_pair in testing_list:
        input_tensor=torch.tensor(testing_pair[0],device=device).view(-1,1)
        target_tensor=torch.tensor(testing_pair[1],device=device).view(-1,1)
        testing_tensor_list.append((input_tensor,target_tensor))
    """
    model
    """
    encoder=EncoderRNN(vocab_size,hidden_size).to(device)
    decoder=DecoderRNN(vocab_size,hidden_size).to(device)
    
    if load_pretrained:
        encode_path = os.path.join('models',f'encoder_teacher{teacher_forcing_ratio:.2f}_hidden{hidden_size}.pt')
        decode_path = os.path.join('models',f'decoder_teacher{teacher_forcing_ratio:.2f}_hidden{hidden_size}.pt')
        encoder.load_state_dict(torch.load(encode_path))
        decoder.load_state_dict(torch.load(decode_path))

        predicted_list=evaluateAll(encoder,decoder,testing_tensor_list,max_length=data.MAX_LENGTH,device=device)

        score=0
        for i,(_,target) in enumerate(testing_input):
            if not verbose:
                print(f'input:  {_}')
                print(f'target: {target}')
                print(f'pred:   {predict}')
                print('='*28)
            predict=data.idx2seq(predicted_list[i])
            score+=compute_bleu(predict,target)
        score/=len(testing_input)
        print(f'BLEU-4: {score:.2f}')
        
        #evaluate(input_tensor,encoder,decoder,data.MAX_LENGTH,device)
    else:

        """
        train
        """
        epochs=20
        loss_list=[]
        BLEU_list=[]
        best_score=0
        best_encoder_wts,best_decoder_wts=None,None
        for epoch in range(1,epochs+1):
            loss=trainIters(encoder,decoder,training_tensor_list,learning_rate=0.05,max_length=data.MAX_LENGTH,teacher_forcing_ratio=0.5,device=device)
            print(f'epoch{epoch:>2d} loss:{loss:.4f}')
            predicted_list=evaluateAll(encoder,decoder,testing_tensor_list,max_length=data.MAX_LENGTH,device=device)
            # testing using bleu-4
            score=0
            for i,(_,target) in enumerate(testing_input):
                if not verbose:
                    print(f'input:  {_}')
                    print(f'target: {target}')
                    print(f'pred:   {predict}')
                    print('='*28)
                predict=data.idx2seq(predicted_list[i])
                score+=compute_bleu(predict,target)
            score/=len(testing_input)
            print(f'BLEU-4: {score:.2f}')

            loss_list.append(loss)
            BLEU_list.append(score)
            # update best model wts
            if score>best_score:
                best_score=score
                best_encoder_wts=copy.deepcopy(encoder.state_dict())
                best_decoder_wts=copy.deepcopy(decoder.state_dict())

        # save model
        torch.save(best_encoder_wts,os.path.join('models',f'encoder_teacher{teacher_forcing_ratio:.2f}_hidden{hidden_size}.pt'))
        torch.save(best_decoder_wts,os.path.join('models',f'decoder_teacher{teacher_forcing_ratio:.2f}_hidden{hidden_size}.pt'))
    
    
        # plot
        fig, ax1 = plt.subplots()
        x_epoch = torch.arange(1, epochs+1, 1)
        
        color = 'tab:red'
        ax1.set_xlabel('epochs')
        ax1.set_ylabel('BLEU-4', color=color)
        ax1.plot(x_epoch, BLEU_list, color=color)
        ax1.tick_params(axis='y', labelcolor=color)

        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis

        color = 'tab:blue'
        ax2.set_ylabel('CrossEntropyLloss', color=color)  # we already handled the x-label with ax1
        ax2.plot(x_epoch, loss_list, color=color)
        ax2.tick_params(axis='y', labelcolor=color)

        fig.tight_layout()  # otherwise the right y-label is slightly clipped
        fig.savefig(os.path.join('result',f'teacher{teacher_forcing_ratio:.2f}_hidden{hidden_size}.png'))
        plt.show()
    


